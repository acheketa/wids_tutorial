{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WiDS Tel Aviv\n",
    "## Tutorial â€“ Dealing with the Lack of Audio Data\n",
    "In recent years, speech data is receiving spotlight for various applications in deep learning, from Automatic Speech Recognition (ASR) system to source separation. And yet, there are not many augmentation techniques explored for speech data compared to those of image data. Thus, in this track, we will explore various methods to augment speech data. This hands-on tutorial will work along the task of building a simple speech classifier with the Speech Commands Zero to Nine (SC09) dataset available by TensorFlow and go over traditional augmentation techniques, transfer learning, GAN augmentation, and style transfer to increase the classification accuracy. Participants are required to download the libraries and pre-trained models, which will be available in late-January."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data\n",
    "* Wave file to spectrogram image\n",
    "* Create text file that contains path information of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample wav file\n",
    "# Listen to it\n",
    "# How many data in each directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert that sample wav file into spectrogram image\n",
    "# This is not for real training, so it shouldn't handle all files (classes)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "classes = os.listdir('./gender_voice')\n",
    "modes = ['']\n",
    "LEN = 16000\n",
    "for label in classes:\n",
    "    for mode in modes:\n",
    "        path = os.path.join('./gender_voice', mode, label)\n",
    "        waves = os.listdir(path)\n",
    "        target_path = os.path.join('./gender_voice_img', mode, label)\n",
    "\n",
    "        for wav_file in waves:\n",
    "            if wav_file[-3:] == 'wav':\n",
    "                rate, data = wavfile.read(os.path.join(path, wav_file))\n",
    "                if len(data) > LEN:\n",
    "                    data = data[:LEN]\n",
    "                else:\n",
    "                    data = np.pad(data, (0, max(0, LEN - len(data))), \"constant\")\n",
    "                fig,ax = plt.subplots(1)\n",
    "                fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "                ax.axis('off')\n",
    "                pxx, freqs, bins, im = ax.specgram(x=data, Fs=rate, noverlap=384, NFFT=512)\n",
    "                ax.axis('off')\n",
    "                fig.savefig(os.path.join(target_path, wav_file)[:-4] + '.jpg', dpi=300, frameon='false')\n",
    "                plt.close()\n",
    "                print(\"Modification complete for \", os.path.join(target_path, wav_file)[:-4] + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample directory image files (several files per class)\n",
    "# Use that to make a text file list\n",
    "# And then load it to show\n",
    "import os\n",
    "\n",
    "labels = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "#labels = os.listdir('./sc09_img/sc09_cyclegan')\n",
    "modes = ['sc09_cyclegan']\n",
    "\n",
    "for mode in modes:\n",
    "    data_path = '/home/data/speech_commands/sc09_img/' + mode\n",
    "    f = open('./' + mode +'.txt', 'w+')\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        file_path = os.path.join(data_path, label)\n",
    "        waves = os.listdir(file_path)\n",
    "\n",
    "        for wave in waves:\n",
    "            wav_path = os.path.join(file_path, wave)\n",
    "            f.write(wav_path + ' ' + str(idx) + '\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it to test/inference mode\n",
    "# Load checkpoint for baseline model\n",
    "# Show Tensorboard for training and evaluation\n",
    "# Describe the code in presentation\n",
    "# Let people inference with sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding noise to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to one sample data\n",
    "# Listen to before and after\n",
    "# Show spectrogram images before and after\n",
    "# Tell them and show Tensorboard for noisy training\n",
    "# Let them inference with both noisy and unnoisy data\n",
    "import random\n",
    "from scipy.io import wavfile\n",
    "from tensorflow.python.platform import gfile\n",
    "import numpy as np\n",
    "\n",
    "LEN = 16000\n",
    "NOISE_VOLUME = 0.1\n",
    "NOISE_FREQUENCY = 0.8\n",
    "\n",
    "# call noises.wav\n",
    "noise_paths = '/home/data/speech_commands/etc/_background_noise_/*.wav'\n",
    "noises = []\n",
    "for noise_path in gfile.Glob(noise_paths):\n",
    "    #noise = librosa.core.load(noise_path, sr=None, duration=1)[0] * NOISE_VOLUME\n",
    "    noise = wavfile.read(noise_path)[1][:LEN] * NOISE_VOLUME\n",
    "    noises.append(noise)\n",
    "\n",
    "# call train audio\n",
    "wav_path = './sc09_wav/train/*/*.wav'\n",
    "waves = gfile.Glob(wav_path)\n",
    "random.shuffle(waves)\n",
    "\n",
    "# add background noise\n",
    "for idx, wave in enumerate(waves):\n",
    "    data = wavfile.read(wave)[1]\n",
    "    if idx <= len(waves) * NOISE_FREQUENCY:\n",
    "        #data, _ = librosa.core.load(wave, sr=None, duration=1)\n",
    "        if len(data) < LEN:\n",
    "            data = np.pad(data, (0, max(0, LEN - len(data))), \"constant\")\n",
    "        else:\n",
    "            data = data[:LEN]\n",
    "        index = random.randint(0, 4)\n",
    "        noise_data = noises[index]\n",
    "        wavfile.write('./sc09_wav/aug/' + wave[17:], LEN, data + noise_data)\n",
    "    else:\n",
    "        #librosa.output.write_wav('./sc09_wav/aug/' + wave[17:], file, sr=LEN)\n",
    "        wavfile.write('./sc09_wav/aug/' + wave[17:], LEN, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is transfer learning\n",
    "# Let's see directories of Speech Command dataset without 0~9\n",
    "# Listen to some sample files\n",
    "# Show SC training by adjusting num_classes\n",
    "# Inference with SC09 --> bad eval result\n",
    "# Now fine tuning model layer is somewhat different\n",
    "\n",
    "# Let people load checkpoint and turn it into numpy file\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "from model import AlexNetModel\n",
    "\n",
    "\n",
    "# Edit just these\n",
    "FILE_PATH = '/home/finetune/training/alexnet_20190220_005707/checkpoint/model_epoch7.ckpt'\n",
    "NUM_CLASSES = 20\n",
    "OUTPUT_FILE = 'sc_epoch7.npy'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = tf.placeholder(tf.float32, [128, 227, 227, 3])\n",
    "    model = AlexNetModel(num_classes=NUM_CLASSES)\n",
    "    model.inference(x)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    layers = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc8']\n",
    "    data = {\n",
    "        'conv1': [],\n",
    "        'conv2': [],\n",
    "        'conv3': [],\n",
    "        'conv4': [],\n",
    "        'conv5': [],\n",
    "        'fc8': []\n",
    "    }\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, FILE_PATH)\n",
    "\n",
    "        for op_name in layers:\n",
    "          with tf.variable_scope(op_name, reuse = True):\n",
    "            biases_variable = tf.get_variable('biases')\n",
    "            weights_variable = tf.get_variable('weights')\n",
    "            data[op_name].append(sess.run(biases_variable))\n",
    "            data[op_name].append(sess.run(weights_variable))\n",
    "\n",
    "        np.save(OUTPUT_FILE, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning.py\n",
    "# model4tl.py --> different model architecture (addition of fc layers)\n",
    "# train_layers for finetuning (optimizer) & skip layers for newly added layers (weight load)\n",
    "# This will be done in the presentation\n",
    "# And then show finetuned training results\n",
    "# Inference with SC09 --> good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is DCGAN\n",
    "# Why generate\n",
    "# Results unstable\n",
    "# Not going to be done here (Tensorboard)\n",
    "# So the solution? CycleGAN loss (more stable) --> CycleGAN generation needs \"features\" --> here \"gender\"\n",
    "# Explain CycleGAN in the presentation\n",
    "# Gender dataset load it, explain it, listen to it, compare & contrast two classes\n",
    "# Tensorboard --> inference (generation) --> use as training data --> f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
